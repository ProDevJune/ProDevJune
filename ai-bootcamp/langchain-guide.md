---
title: 'LangChain ì™„ë²½ ê°€ì´ë“œ'
layout: page
icon: fas fa-link
permalink: /ai-bootcamp/langchain-guide/
toc: true
tags:
  - LangChain
  - LLM
  - ëŒ€í˜•ì–¸ì–´ëª¨ë¸
  - RAG
  - ì²´ì¸
  - ì—ì´ì „íŠ¸
  - í”„ë¡¬í”„íŠ¸ì—”ì§€ë‹ˆì–´ë§
  - OpenAI
  - ë²¡í„°ë°ì´í„°ë² ì´ìŠ¤
  - íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤
  - íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤AIë¶€íŠ¸ìº í”„
  - ì—…ìŠ¤í…Œì´ì§€íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤
  - UpstageAILab
  - êµ­ë¹„ì§€ì›
  - íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ì—…ìŠ¤í…Œì´ì§€ì—ì´ì•„ì´ë©
  - íŒ¨ìŠ¤íŠ¸ìº í¼ìŠ¤ì—…ìŠ¤í…Œì´ì§€ë¶€íŠ¸ìº í”„
---

# ğŸ”— LangChain ì™„ë²½ ê°€ì´ë“œ

## ğŸ“š ê°œìš”

LangChainì€ ëŒ€í˜•ì–¸ì–´ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì„ ìœ„í•œ ê°•ë ¥í•œ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ë³µì¡í•œ AI ì›Œí¬í”Œë¡œìš°ë¥¼ ì²´ì¸ í˜•íƒœë¡œ êµ¬ì„±í•˜ì—¬ ë”ìš± ì •êµí•˜ê³  ì‹¤ìš©ì ì¸ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.

---

## ğŸ”¹ 1. LangChain ê¸°ì´ˆ ê°œë…

### ğŸ“Œ LangChainì´ë€?

LangChainì€ LLMì„ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ í”„ë ˆì„ì›Œí¬ë¡œ, ë‹¤ìŒê³¼ ê°™ì€ í•µì‹¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:

**ì£¼ìš” íŠ¹ì§•:**
- **ì²´ì¸(Chain)**: ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±
- **í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿**: ì¬ì‚¬ìš© ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬
- **ë©”ëª¨ë¦¬**: ëŒ€í™” íˆìŠ¤í† ë¦¬ ë° ìƒíƒœ ê´€ë¦¬
- **ì—ì´ì „íŠ¸**: ë„êµ¬ í™œìš© ë° ì¶”ë¡  ê¸°ë°˜ ì‘ì—… ìˆ˜í–‰
- **RAG**: ì™¸ë¶€ ì§€ì‹ ì†ŒìŠ¤ì™€ LLM ì—°ê²°

### ğŸ“Œ ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •

```bash
# Python ì„¤ì¹˜
pip install langchain
pip install openai
pip install chromadb
pip install tiktoken
```

```python
import os
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

# API í‚¤ ì„¤ì •
os.environ['OPENAI_API_KEY'] = 'your-api-key'

# LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
llm = OpenAI(temperature=0.7)
chat_model = ChatOpenAI(temperature=0.7)
```

---

## ğŸ”¹ 2. í•µì‹¬ ì»´í¬ë„ŒíŠ¸

### ğŸ“Œ 2.1 í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Prompt Templates)

**ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:**
```python
from langchain import PromptTemplate

# í…œí”Œë¦¿ ì •ì˜
template = """
ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ì§ˆë¬¸ì— {style} ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

ì§ˆë¬¸: {question}
ë‹µë³€:
"""

prompt = PromptTemplate(
    input_variables=["role", "style", "question"],
    template=template
)

# í”„ë¡¬í”„íŠ¸ ìƒì„±
formatted_prompt = prompt.format(
    role="íŒŒì´ì¬ ì „ë¬¸ê°€",
    style="ì¹œê·¼í•˜ê³  ìƒì„¸í•˜ê²Œ",
    question="ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
)
```

**ì±„íŒ… í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿:**
```python
from langchain.prompts import ChatPromptTemplate

chat_template = ChatPromptTemplate.from_messages([
    ("system", "ë‹¹ì‹ ì€ {domain} ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
    ("human", "{user_input}")
])

messages = chat_template.format_messages(
    domain="ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤",
    user_input="ë¨¸ì‹ ëŸ¬ë‹ê³¼ ë”¥ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."
)
```

### ğŸ“Œ 2.2 ì²´ì¸ (Chains)

**ê¸°ë³¸ ì²´ì¸:**
```python
from langchain.chains import LLMChain

# LLM ì²´ì¸ ìƒì„±
chain = LLMChain(
    llm=llm,
    prompt=prompt
)

# ì²´ì¸ ì‹¤í–‰
result = chain.run({
    "role": "AI ì—°êµ¬ì",
    "style": "í•™ìˆ ì ìœ¼ë¡œ",
    "question": "Transformer ì•„í‚¤í…ì²˜ì˜ í•µì‹¬ì€ ë¬´ì—‡ì¸ê°€ìš”?"
})
```

**ìˆœì°¨ ì²´ì¸ (Sequential Chain):**
```python
from langchain.chains import SimpleSequentialChain

# ì²« ë²ˆì§¸ ì²´ì¸: ì£¼ì œ ìš”ì•½
summary_template = "ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”: {text}"
summary_prompt = PromptTemplate(
    input_variables=["text"],
    template=summary_template
)
summary_chain = LLMChain(llm=llm, prompt=summary_prompt)

# ë‘ ë²ˆì§¸ ì²´ì¸: ì§ˆë¬¸ ìƒì„±
question_template = "ë‹¤ìŒ ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ í€´ì¦ˆ ë¬¸ì œë¥¼ ë§Œë“œì„¸ìš”: {summary}"
question_prompt = PromptTemplate(
    input_variables=["summary"],
    template=question_template
)
question_chain = LLMChain(llm=llm, prompt=question_prompt)

# ìˆœì°¨ ì²´ì¸ ì—°ê²°
overall_chain = SimpleSequentialChain(
    chains=[summary_chain, question_chain]
)
```

### ğŸ“Œ 2.3 ë©”ëª¨ë¦¬ (Memory)

**ëŒ€í™” ë²„í¼ ë©”ëª¨ë¦¬:**
```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# ë©”ëª¨ë¦¬ ì„¤ì •
memory = ConversationBufferMemory()

# ëŒ€í™” ì²´ì¸
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# ëŒ€í™” ì§„í–‰
response1 = conversation.predict(input="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” íŒŒì´ì¬ì„ ë°°ìš°ê³  ìˆìŠµë‹ˆë‹¤.")
response2 = conversation.predict(input="ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì„ ì•Œë ¤ì£¼ì„¸ìš”.")
```

**ìš”ì•½ ë©”ëª¨ë¦¬:**
```python
from langchain.memory import ConversationSummaryMemory

summary_memory = ConversationSummaryMemory(
    llm=llm,
    max_token_limit=100
)

conversation_with_summary = ConversationChain(
    llm=llm,
    memory=summary_memory,
    verbose=True
)
```

---

## ğŸ”¹ 3. RAG (Retrieval-Augmented Generation)

### ğŸ“Œ 3.1 ë¬¸ì„œ ë¡œë“œ ë° ë¶„í• 

```python
from langchain.document_loaders import TextLoader
from langchain.text_splitter import CharacterTextSplitter

# ë¬¸ì„œ ë¡œë“œ
loader = TextLoader('document.txt')
documents = loader.load()

# ë¬¸ì„œ ë¶„í• 
text_splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
docs = text_splitter.split_documents(documents)
```

### ğŸ“Œ 3.2 ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
embeddings = OpenAIEmbeddings()

# ë²¡í„° ì €ì¥ì†Œ ìƒì„±
vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embeddings
)

# ìœ ì‚¬ë„ ê²€ìƒ‰
query = "ë¨¸ì‹ ëŸ¬ë‹ì˜ ì •ì˜ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
similar_docs = vectorstore.similarity_search(query, k=3)
```

### ğŸ“Œ 3.3 RAG ì²´ì¸ êµ¬ì¶•

```python
from langchain.chains import RetrievalQA

# RAG ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)

# ì§ˆì˜ì‘ë‹µ
result = qa_chain("ë”¥ëŸ¬ë‹ê³¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì°¨ì´ì ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
print(f"ë‹µë³€: {result['result']}")
print(f"ì¶œì²˜: {result['source_documents']}")
```

---

## ğŸ”¹ 4. ì—ì´ì „íŠ¸ (Agents)

### ğŸ“Œ 4.1 ë„êµ¬ ì •ì˜

```python
from langchain.tools import Tool
from langchain.agents import initialize_agent, AgentType
import requests

# ì»¤ìŠ¤í…€ ë„êµ¬ ì •ì˜
def search_wikipedia(query: str) -> str:
    """ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{query}"
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        return data.get('extract', 'ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    return "ê²€ìƒ‰ ì‹¤íŒ¨"

# ë„êµ¬ ë“±ë¡
tools = [
    Tool(
        name="Wikipedia Search",
        func=search_wikipedia,
        description="ìœ„í‚¤í”¼ë””ì•„ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤."
    )
]
```

### ğŸ“Œ 4.2 ì—ì´ì „íŠ¸ ìƒì„± ë° ì‹¤í–‰

```python
# ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
agent = initialize_agent(
    tools=tools,
    llm=llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# ì—ì´ì „íŠ¸ ì‹¤í–‰
response = agent.run("íŒŒì´ì¬ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”.")
```

---

## ğŸ”¹ 5. ì‹¤ì œ í”„ë¡œì íŠ¸ ì˜ˆì‹œ

### ğŸ“Œ 5.1 ë¬¸ì„œ ê¸°ë°˜ QA ì‹œìŠ¤í…œ

```python
from langchain.document_loaders import PyPDFLoader
from langchain.chains.question_answering import load_qa_chain

class DocumentQASystem:
    def __init__(self, pdf_path):
        self.loader = PyPDFLoader(pdf_path)
        self.documents = self.loader.load()
        self.text_splitter = CharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        self.docs = self.text_splitter.split_documents(self.documents)
        
        # ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma.from_documents(
            documents=self.docs,
            embedding=self.embeddings
        )
        
        # QA ì²´ì¸ ì„¤ì •
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=ChatOpenAI(temperature=0),
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever()
        )
    
    def ask_question(self, question):
        return self.qa_chain.run(question)

# ì‚¬ìš© ì˜ˆì‹œ
qa_system = DocumentQASystem("research_paper.pdf")
answer = qa_system.ask_question("ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ì—¬ëŠ” ë¬´ì—‡ì¸ê°€ìš”?")
```

### ğŸ“Œ 5.2 ëŒ€í™”í˜• ì±—ë´‡

```python
from langchain.memory import ConversationBufferWindowMemory

class ConversationalBot:
    def __init__(self):
        self.memory = ConversationBufferWindowMemory(k=5)
        self.llm = ChatOpenAI(temperature=0.7)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        self.system_template = """
        ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
        ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
        
        ì´ì „ ëŒ€í™”:
        {chat_history}
        
        í˜„ì¬ ì§ˆë¬¸: {question}
        ë‹µë³€:
        """
        
        self.prompt = PromptTemplate(
            input_variables=["chat_history", "question"],
            template=self.system_template
        )
        
        self.chain = LLMChain(
            llm=self.llm,
            prompt=self.prompt,
            memory=self.memory
        )
    
    def chat(self, message):
        response = self.chain.predict(question=message)
        return response

# ì‚¬ìš© ì˜ˆì‹œ
bot = ConversationalBot()
print(bot.chat("ì•ˆë…•í•˜ì„¸ìš”! íŒŒì´ì¬ì„ ë°°ìš°ê³  ì‹¶ì–´ìš”."))
print(bot.chat("ì–´ë–¤ ì±…ì„ ì¶”ì²œí•´ì£¼ì‹œë‚˜ìš”?"))
```

---

## ğŸ”¹ 6. ê³ ê¸‰ ê¸°ëŠ¥ ë° ìµœì í™”

### ğŸ“Œ 6.1 ì»¤ìŠ¤í…€ ì²´ì¸ ê°œë°œ

```python
from langchain.chains.base import Chain
from typing import Dict, List

class CustomAnalysisChain(Chain):
    """ì»¤ìŠ¤í…€ ë¶„ì„ ì²´ì¸"""
    
    def __init__(self, llm):
        super().__init__()
        self.llm = llm
    
    @property
    def input_keys(self) -> List[str]:
        return ["text"]
    
    @property
    def output_keys(self) -> List[str]:
        return ["sentiment", "keywords", "summary"]
    
    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:
        text = inputs["text"]
        
        # ê°ì • ë¶„ì„
        sentiment_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°ì •ì„ ë¶„ì„í•˜ì„¸ìš”: {text}"
        sentiment = self.llm.predict(sentiment_prompt)
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”: {text}"
        keywords = self.llm.predict(keyword_prompt)
        
        # ìš”ì•½
        summary_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ìš”ì•½í•˜ì„¸ìš”: {text}"
        summary = self.llm.predict(summary_prompt)
        
        return {
            "sentiment": sentiment,
            "keywords": keywords,
            "summary": summary
        }
```

### ğŸ“Œ 6.2 ì„±ëŠ¥ ìµœì í™”

```python
# ìºì‹± í™œìš©
from langchain.cache import InMemoryCache
import langchain
langchain.llm_cache = InMemoryCache()

# ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

streaming_llm = ChatOpenAI(
    temperature=0.7,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()]
)

# ë¹„ë™ê¸° ì²˜ë¦¬
import asyncio
from langchain.chains import LLMChain

async def async_chain_run():
    chain = LLMChain(llm=llm, prompt=prompt)
    result = await chain.arun({
        "role": "ê°œë°œì",
        "style": "ê°„ë‹¨í•˜ê²Œ",
        "question": "REST APIë€ ë¬´ì—‡ì¸ê°€ìš”?"
    })
    return result
```

---

## ğŸ”¹ 7. ì‹¤ìŠµ í”„ë¡œì íŠ¸ ë° ì‘ìš©

### ğŸ“Œ 7.1 ì¶”ì²œ í”„ë¡œì íŠ¸

1. **ê°œì¸ ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ**
   - PDF, í…ìŠ¤íŠ¸ íŒŒì¼ì„ ë²¡í„°í™”
   - ìì—°ì–´ ì§ˆì˜ë¡œ ë¬¸ì„œ ê²€ìƒ‰
   - RAG ê¸°ë°˜ ì •í™•í•œ ë‹µë³€ ìƒì„±

2. **ì—…ë¬´ ìë™í™” ë´‡**
   - ì´ë©”ì¼ ìš”ì•½ ë° ë¶„ë¥˜
   - íšŒì˜ë¡ ìƒì„± ë° ì•¡ì…˜ ì•„ì´í…œ ì¶”ì¶œ
   - ë³´ê³ ì„œ ìë™ ìƒì„±

3. **í•™ìŠµ ë„ìš°ë¯¸ ì‹œìŠ¤í…œ**
   - êµì¬ ê¸°ë°˜ QA ì‹œìŠ¤í…œ
   - í€´ì¦ˆ ìƒì„± ë° í‰ê°€
   - ê°œì¸í™”ëœ í•™ìŠµ ê³„íš ìˆ˜ë¦½

### ğŸ“Œ 7.2 ë””ë²„ê¹… ë° ë¬¸ì œí•´ê²°

**ì¼ë°˜ì ì¸ ë¬¸ì œë“¤:**
```python
# 1. í† í° ê¸¸ì´ ì´ˆê³¼ ë¬¸ì œ
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", " ", ""]
)

# 2. API ë¹„ìš© ìµœì í™”
from langchain.llms import OpenAI

# ë” ì €ë ´í•œ ëª¨ë¸ ì‚¬ìš©
cheap_llm = OpenAI(model_name="gpt-3.5-turbo-instruct", temperature=0)

# 3. ì‘ë‹µ ì†ë„ ê°œì„ 
from langchain.cache import SQLiteCache
langchain.llm_cache = SQLiteCache(database_path=".langchain.db")
```

---

## ğŸ”¹ 8. ìµœì‹  ë™í–¥ ë° ë°œì „ ë°©í–¥

### ğŸ“Œ 8.1 LangChain Expression Language (LCEL)

```python
from langchain.schema.runnable import RunnableLambda
from langchain.schema.output_parser import StrOutputParser

# LCEL ì²´ì¸ êµ¬ì„±
chain = (
    prompt
    | llm
    | StrOutputParser()
)

# ë³‘ë ¬ ì²˜ë¦¬
from langchain.schema.runnable import RunnableParallel

parallel_chain = RunnableParallel({
    "summary": summary_chain,
    "translation": translation_chain
})
```

### ğŸ“Œ 8.2 í–¥í›„ í•™ìŠµ ë°©í–¥

1. **LangSmith**: í”„ë¡œë•ì…˜ ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…
2. **LangServe**: API ì„œë²„ ë°°í¬ ë° ì„œë¹™
3. **Multi-modal**: ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ í†µí•© ì²˜ë¦¬
4. **Fine-tuning**: ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸ í•™ìŠµ ì—°ê³„

---

## ğŸ’¡ ì •ë¦¬ ë° ë‹¤ìŒ ë‹¨ê³„

### ğŸ¯ í•µì‹¬ ìš”ì 
- **LangChainì€ LLM ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œì˜ í‘œì¤€ í”„ë ˆì„ì›Œí¬**
- **ì²´ì¸, ë©”ëª¨ë¦¬, ì—ì´ì „íŠ¸ë¥¼ í†µí•œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì„±**
- **RAGë¥¼ í†µí•œ ì™¸ë¶€ ì§€ì‹ í™œìš© ë° ì •í™•ë„ í–¥ìƒ**
- **í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œì˜ ì„±ëŠ¥ ìµœì í™” í•„ìš”**

### ğŸ“š ì¶”ê°€ í•™ìŠµ ìë£Œ
- [LangChain ê³µì‹ ë¬¸ì„œ](https://docs.langchain.com/)
- [LangChain Cookbook](https://github.com/langchain-ai/langchain/tree/master/cookbook)
- [LangSmith í”Œë«í¼](https://smith.langchain.com/)

### ğŸš€ ì‹¤ìŠµ ê³¼ì œ
1. ê°œì¸ ë¬¸ì„œ ì»¬ë ‰ì…˜ìœ¼ë¡œ RAG ì‹œìŠ¤í…œ êµ¬ì¶•
2. ë©€í‹° ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° ì„¤ê³„
3. ì»¤ìŠ¤í…€ ë„êµ¬ ê°œë°œ ë° í†µí•©
4. í”„ë¡œë•ì…˜ ë°°í¬ ë° ëª¨ë‹ˆí„°ë§ êµ¬í˜„

---

*ì´ ê°€ì´ë“œëŠ” LangChain 0.1+ ë²„ì „ì„ ê¸°ì¤€ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë  ì˜ˆì •ì…ë‹ˆë‹¤.*
